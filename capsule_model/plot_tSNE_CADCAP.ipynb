{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67fbfb41-6efb-402b-ba03-c3af8f859342",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/23 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat2 in method wrapper_mm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 207>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    206\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 208\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    171\u001b[0m model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(args\u001b[38;5;241m.\u001b[39mgpu))\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m#-------Get representations-------#\u001b[39;00m\n\u001b[0;32m--> 174\u001b[0m train_embeddings, train_labels \u001b[38;5;241m=\u001b[39m \u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m test_embeddings, test_labels \u001b[38;5;241m=\u001b[39m get_embeddings(test_loader)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m#-------Use t-SNE visualization-------#\u001b[39;00m\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mget_embeddings\u001b[0;34m(loader)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j,(images, targets) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(loader)):\n\u001b[1;32m    147\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 148\u001b[0m     outputs, embeddings_batch \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m     embeddings\u001b[38;5;241m.\u001b[39mappend(embeddings_batch\u001b[38;5;241m.\u001b[39mcpu())\n\u001b[1;32m    150\u001b[0m     labels\u001b[38;5;241m.\u001b[39mappend(targets\u001b[38;5;241m.\u001b[39mcpu())\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/capsule_model/resnext50.py:331\u001b[0m, in \u001b[0;36mResNeXt_mpa.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    329\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    330\u001b[0m embedding \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m--> 331\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory_prototype_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquerys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m pae_embedding\u001b[38;5;241m=\u001b[39mx\n\u001b[1;32m    333\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/capsule_model/resnext50.py:271\u001b[0m, in \u001b[0;36mResNeXt_mpa.memory_prototype_attention\u001b[0;34m(self, querys)\u001b[0m\n\u001b[1;32m    268\u001b[0m prototype_normalized \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnormalize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprototype, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m# 计算查询与多个原型之间的余弦相似度\u001b[39;00m\n\u001b[0;32m--> 271\u001b[0m attention_score \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprototype_normalized\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    273\u001b[0m \u001b[38;5;66;03m# 如果 attention_score 的总和为 0，则跳过\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msum(attention_score) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat2 in method wrapper_mm)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "import argparse\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from torch.utils.data import random_split,Dataset,DataLoader\n",
    "from prettytable import PrettyTable\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sklearn.metrics import f1_score\n",
    "from resnext50 import ResNeXt50_32x4d\n",
    "from pytorch_metric_learning import losses, miners\n",
    "from pytorch_metric_learning.distances import SNRDistance\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "from sklearn.manifold import TSNE\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import seaborn as sns\n",
    "\n",
    "# get all args for the learning process\n",
    "\n",
    "torch.set_printoptions(sci_mode=False) # close sci_model like 0.001 instead of 1e-3\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "# get all args for the learning process\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Args for CADCAP\")\n",
    "    parser.add_argument(\"--dataset\",\n",
    "                        type=str,\n",
    "                        default=\"CADCAP\",\n",
    "                        help=\"DATASETNAME'\")\n",
    "    parser.add_argument(\"--datadir\",\n",
    "                        type=str,\n",
    "                        default=\"../data/MICCAI_wcetraining811\",\n",
    "                        help=\"Dir of your dataset, like '/data/DATASETNAME'\")\n",
    "    parser.add_argument(\"--num_workers\",\n",
    "                        type=int,\n",
    "                        default=12,\n",
    "                        help=\"Number of workers for loading the data\")\n",
    "    parser.add_argument(\"--model\",\n",
    "                        type=str,\n",
    "                        default=\"ResNext50_32x4d\",\n",
    "                        help=\"Model name, default=ResNeXt50_32x4d\")\n",
    "    parser.add_argument(\"--num_classes\",\n",
    "                        type=int,\n",
    "                        default=3,\n",
    "                        help=\"Number of classes, default: 3\")\n",
    "    parser.add_argument(\"--batch_size\",\n",
    "                        type=int,\n",
    "                        default=64,\n",
    "                        help=\"Batch size, default=64\")\n",
    "    parser.add_argument(\"--seed\",\n",
    "                        type=int,\n",
    "                        default=42,\n",
    "                        help=\"Random seed, default=42\")\n",
    "    parser.add_argument(\"--epochs\",\n",
    "                        type=int,\n",
    "                        default=30,\n",
    "                        help=\"Max training epochs, default=50\")\n",
    "    parser.add_argument(\"--optimizer\",\n",
    "                        type=str,\n",
    "                        default=\"SGD\",\n",
    "                        help=\"optimizer, default=SGD\")\n",
    "    parser.add_argument(\"--lr\",\n",
    "                        type=float,\n",
    "                        default=0.01,\n",
    "                        help=\"learning rate (default: 0.01)\")\n",
    "    parser.add_argument(\"--weight_decay\",\n",
    "                        type=float,\n",
    "                        default=0.0000,\n",
    "                        help=\"weight-decay (default: 0.0000)\")\n",
    "    parser.add_argument(\"--gpu\",\n",
    "                        type=str,\n",
    "                        default=\"0\",\n",
    "                        help=\"input visible devices for training (default: cuda:0)\")\n",
    "    args = parser.parse_args(args=[])\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n",
    "    return args\n",
    "\n",
    "\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "def load_data(_args):\n",
    "    dataset_dir = _args.datadir\n",
    "    train_dir = os.path.join(dataset_dir, 'train')\n",
    "    valid_dir = os.path.join(dataset_dir, 'valid')\n",
    "    test_dir = os.path.join(dataset_dir, 'test')\n",
    "\n",
    "    image_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.3663, 0.2485, 0.1391),\n",
    "                                 std =(0.2628, 0.1856, 0.1255))\n",
    "        ]),\n",
    "        'valid': transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.3663, 0.2485, 0.1391),\n",
    "                                 std =(0.2628, 0.1856, 0.1255))\n",
    "        ]),\n",
    "        'test': transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.3663, 0.2485, 0.1391),\n",
    "                                 std =(0.2628, 0.1856, 0.1255))\n",
    "        ])\n",
    "    }\n",
    "    \n",
    "    \n",
    "    train_dataset=datasets.ImageFolder(train_dir,image_transforms['train'])\n",
    "    valid_dataset=datasets.ImageFolder(valid_dir,image_transforms['valid'])\n",
    "    test_dataset=datasets.ImageFolder(test_dir,image_transforms['test'])\n",
    "    \n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=_args.batch_size, num_workers=_args.num_workers, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=_args.batch_size, num_workers=_args.num_workers, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=_args.batch_size, num_workers=_args.num_workers, shuffle=False)\n",
    "\n",
    "    train_data_size = len(train_dataset)\n",
    "    valid_data_size = len(valid_dataset)\n",
    "    test_data_size = len(test_dataset)\n",
    "    \n",
    "    # class_weight = get_weight(train_dataset, _args)\n",
    "    \n",
    "    return train_loader, valid_loader, test_loader, train_data_size, valid_data_size, test_data_size\n",
    "\n",
    "        \n",
    "def get_embeddings(loader):\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for j,(images, targets) in enumerate(tqdm(loader)):\n",
    "            images = images.to(\"cuda:0\")\n",
    "            outputs, embeddings_batch = model(images)\n",
    "            embeddings.append(embeddings_batch.cpu())\n",
    "            labels.append(targets.cpu())\n",
    "    embeddings = torch.cat(embeddings, dim=0)\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "    return embeddings, labels\n",
    "\n",
    "\n",
    "def main():\n",
    "    #-------Load hyperparameters-------#\n",
    "    args = get_args()\n",
    "    setup_seed(seed=args.seed)\n",
    "\n",
    "    \n",
    "    #-------Load data-------#\n",
    "    global train_data_size, valid_data_size, test_data_size\n",
    "    train_loader, valid_loader, test_loader, train_data_size, valid_data_size, test_data_size = load_data(args)\n",
    "    #-------Create model-------#\n",
    "    model_file=\"./model/BEST_datasetCADCAP3classes811_testacc0.9891_f1score0.9891_Triplet_modelResNext50_32x4d_bs64_seed42_epochs70_optimSGD_lr0.01_wd0.0.pt\"\n",
    "    global model\n",
    "    model = torch.load(model_file)\n",
    "    \n",
    "    #--------Use cuda------#\n",
    "    model.to(\"cuda:{}\".format(args.gpu))\n",
    "    \n",
    "    #-------Get representations-------#\n",
    "    train_embeddings, train_labels = get_embeddings(train_loader)\n",
    "    test_embeddings, test_labels = get_embeddings(test_loader)\n",
    "    \n",
    "    #-------Use t-SNE visualization-------#\n",
    "    tsne = TSNE(n_components=2)\n",
    "    train_embeddings_tsne = tsne.fit_transform(train_embeddings)\n",
    "    test_embeddings_tsne = tsne.fit_transform(test_embeddings)\n",
    "    plt.style.use('ggplot')\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    \n",
    "    #-------Plot-------#\n",
    "    label_list = [\"Inflammatory\", \"Normal\", \"Vascular Lesions\"]\n",
    "    train_labels = [label_list[index] for index in train_labels]\n",
    "    test_labels = [label_list[index] for index in test_labels]\n",
    "    sns.scatterplot(x=train_embeddings_tsne[:, 0], y=train_embeddings_tsne[:, 1], hue=train_labels, palette='Dark2', hue_order=label_list)\n",
    "    plt.title('Representations on the training set')\n",
    "    \n",
    "    plt.axhline(0, color='black', linestyle=':', linewidth=0.8)  # y=0 轴线 magenta cyan\n",
    "    plt.axvline(0, color='black', linestyle=':', linewidth=0.8)  # x=0 轴线\n",
    "    plt.axis('on')  # 确保坐标轴可见\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.scatterplot(x=test_embeddings_tsne[:, 0], y=test_embeddings_tsne[:, 1], hue=test_labels, palette='Dark2', hue_order=label_list)\n",
    "    plt.title('Representations on the test set')\n",
    "    \n",
    "    plt.axhline(0, color='black', linestyle=':', linewidth=0.8)  # y=0 轴线\n",
    "    plt.axvline(0, color='black', linestyle=':', linewidth=0.8)  # x=0 轴线\n",
    "    plt.axis('on')  # 确保坐标轴可见\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"./figures/{args.dataset}ours_train_test_tSNE.png\",dpi=600)\n",
    "    plt.show()\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e875465-4c16-45cb-a7f2-802d1939f0e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
